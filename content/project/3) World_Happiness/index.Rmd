---
title: "World Happiness"
author: "Harsh Tripathi"
summary: "A project involving data analysis and modeling."
tags:
- Regression Modeling
date: "2021-09-16T00:00:00Z"
image: 
  focal_point: Smart

output:
  html_document:
    theme: flatly
    highlight: zenburn
    number_sections: yes
    toc: yes
    toc_float: yes
    code_folding: show
---


```{r, setup, echo=FALSE}
knitr::opts_chunk$set(
  message = FALSE, 
  warning = FALSE, 
  tidy=FALSE,     # display code as typed
  size="small")   # slightly smaller font for code
options(digits = 3)

# default figure size
knitr::opts_chunk$set(
  fig.width=6.75, 
  fig.height=6.75,
  fig.align = "center"
)
```


```{r load-libraries, echo=FALSE}
library(tidyverse)
library(mosaic)
library(ggthemes)
library(GGally)
library(readxl)
library(here)
library(httr)
library(skimr)
library(janitor)
library(broom)
library(tidyquant)
library(infer)
library(openintro)
library(huxtable)
```
In this project, we work on the world happiness data which measures life satisfaction of residents of various countries along with a bunch of other variables such as social support, life expectancy and GDP per capita. We start with pulling the data, manipulating it, doing exploratory data analysis and finally building a regression model.


```{r, loading_data}
url <- "https://happiness-report.s3.amazonaws.com/2021/DataPanelWHR2021C2.xls"

#Download data to temporary file
httr::GET(url, write_disk(happiness.temp <- tempfile(fileext = ".xls")))

# Use read_excel to read it as dataframe
world_happiness <- read_excel(happiness.temp,
                    sheet = "Sheet1",
                    range = cell_cols("A:K")) %>% 
  janitor::clean_names()
```

The variable of interest here is life_ladder, which indicates how happy the residents of a country are. A score of 10 reflects that the respondent feels that they are living the best possible life while a score of 0 signifies that they are living the worst possible life.

For our analysis, we will consider data of 2019.


## Exploratory Data Analysis

```{r, extract_data_and_skim}
#Considering data of 2019
world_happiness_19 <- world_happiness %>% 
  filter(year == 2019)

skim(world_happiness_19)
```


```{r, data_for_world_map_plot}
library(countrycode) #To clean up country names

#Getting country_iso3 code for each country
country_iso3 <-  world_happiness_19 %>% 
  select(country_name) %>% 
  pull() %>% 
  countrycode(
    origin = "country.name",
    destination = 'iso3c') %>% 
  as_tibble()

#Bind them with right country codes
plot_data <- bind_cols(world_happiness_19, country_iso3)
```


```{r, visualize_data_on_world_map}
library(rworldmap) #To get the world map
library(RColorBrewer) #To get a colour palette

#Joining our data with rworldmap's data for plotting
joinData <- joinCountryData2Map(plot_data,
                                joinCode = "ISO3",
                                nameJoinColumn = "value")

#Setting a colour palette
colourPalette <- RColorBrewer::brewer.pal(5,"YlGnBu")

#Plotting a world map with life_ladder
theMap <- mapCountryData(joinData, 
                         nameColumnToPlot = "life_ladder",
                         addLegend = FALSE,
                         colourPalette = colourPalette,
                         mapTitle = "Average life ladder scores - 2019")

do.call(addMapLegend, c(theMap, legendWidth=1, legendMar = 2))
```

We observe that developed countries such as America, Australia and Nordic countries are the happiest in the world. On the other hand, African countries such as Rwanda and Zimbabwe have the least life_ladder score.


```{r, glimpse_data}
glimpse(world_happiness_19)
```

```{r, scatterplot_correlation_matrix, fig.width=16}
#Create a scatterplot correlation matrix
world_happiness_19 %>% 
  select(-c(country_name,year)) %>% 
  ggpairs(alpha = 0.3)
```



```{r, getting_continent}
#Getting continent for each country
continent_list <-  world_happiness_19 %>% 
  select(country_name) %>% 
  pull() %>% 
  countrycode(
    origin = "country.name",
    destination = 'continent') %>% 
  as_tibble()

#Binding continent with data
world_happiness_19 <- bind_cols(world_happiness_19, continent_list)

#Renaming continent column
world_happiness_19 <- rename(world_happiness_19, continent=value)
```

```{r, happiness_distribution_by_continent}
world_happiness_19 %>% 
  
  #Filtering out cases where continent is NA
  filter(!is.na(continent)) %>% 
  
  #Histogram of life_ladder_score by continent
  ggplot(aes(x=life_ladder, fill = continent)) +
  geom_histogram(alpha = 0.5) +
  
  #Black and white theme
  theme_bw() +
  
  #Putting title and axes labels
  labs(title = "Histogram of Life Satisfaction by Continents",
       x = "Life Ladder Score",
       y = "Count of countries") 
```

It can be observed from the plot that European countries have the highest life ladder score on average while African countries have the lowest. Asia has the maximum range of life ladder scores.


```{r, happiness_by_gdp}
world_happiness_19 %>% 
  
  #Scatterplot between happiness and gdp, coloured by continent
  ggplot(aes(x=log_gdp_per_capita, y=life_ladder, col=continent)) +
  geom_point() +
  
  #Black and white theme
  theme_bw() +
  
  #Putting title and axes labels
  labs(title = "Scatterplot between Life Satisfaction and Log GDP Per Capita",
       x = "Log GDP Per Capita",
       y = "Life Ladder Score",
       #Renaming the legend
       color = "Continents")
```

It can be observed that there is a strong positive correlation between Life Ladder Score and GDP Per Capita. On average, people in countries with high GDP per capita tend to be happier. This holds true for every continent except Africa where the variability in data is very high.


```{r, happiness_by_social_support}
world_happiness_19 %>% 
  
  #Scatterplot between happiness and social_support, coloured by continent
  ggplot(aes(x=social_support, y=life_ladder, col=continent)) +
  geom_point() +
  
  #Black and white theme
  theme_bw() +
  
  #Putting title and axes labels
  labs(title = "Scatterplot between Life Satisfaction and Social Support",
       x = "Social Support",
       y = "Life Ladder Score",
       #Renaming the legend
       color = "Continents")
```

It can be observed that there is a strong positive correlation between Life Ladder Score and Social Support. On average, people having higher social support tend to be happier. This holds true for every continent except Africa where the variability in data is very high.


```{r, happiness_by_life_exp}
world_happiness_19 %>% 
  
  #Scatterplot between happiness and life exp, coloured by continent
  ggplot(aes(x=healthy_life_expectancy_at_birth, y=life_ladder, col=continent)) +
  geom_point() +
  
  #Black and white theme
  theme_bw() +
  
  #Putting title and axes labels
  labs(title = "Scatterplot between Life Satisfaction and Life Expectancy",
       x = "Life Expectancy at Birth",
       y = "Life Ladder Score",
       #Renaming the legend
       color = "Continents")
```

It can be observed that there is a strong positive correlation between Life Ladder Score and Life Expectancy at Birth. On average, people in countries with high life expectancy tend to be happier. This holds true for every continent except Africa where the variability in data is very high.


Overall, through our EDA, it seems GDP per capita, social support and life expectancy would be important variables in our regression model as they are highly correlated with the dependent variable (life ladder).


## Model building

```{r, drop_missing_values}
#As the count of missing values is not significant, dropping them
world_happiness_19_complete <- world_happiness_19 %>% 
  drop_na()


#Converting continent to factor variable
world_happiness_19_complete <- world_happiness_19_complete %>% 
  mutate(continent = factor(continent))

skim(world_happiness_19_complete)
```

```{r, train_test_split}
#Splitting the data into train and test sets

library(rsample) #Package to split data
set.seed(1234)

#Keeping 75% data in the training set
train_test_split <- initial_split(world_happiness_19_complete, prop = 0.75)

train_data <- training(train_test_split)
test_data <- testing(train_test_split)
```

We will run the first iteration of the model with only log GDP per capita as the explanatory variable.


```{r, model1}
#Linear regression model 
model1 <- lm(life_ladder ~ log_gdp_per_capita, data = train_data)

#Summary of the model
mosaic::msummary(model1)

#RMSE on the training set
rmse_train <- train_data %>% 
  mutate(predictions = predict(model1, .)) %>% 
  summarise(
    sqrt(sum(predictions - life_ladder)**2 / n())
  ) %>% 
  pull()

cat(paste0("\nRMSE on training set is: ", round(rmse_train, 4)))

#RMSE on the testing set
rmse_test <- test_data %>% 
  mutate(predictions = predict(model1, .)) %>% 
  summarise(
    sqrt(sum(predictions - life_ladder)**2 / n())
  ) %>% 
  pull()

cat(paste0("\nRMSE on testing set is: ", round(rmse_test, 4)))
```

We observe that if we only use GDP to predict life ladder, the model explains 63% variability in the data and the RMSE on test data is 0.3365.

Now, we add life expectancy to the model.

```{r, model2}
#Linear regression model 
model2 <- lm(life_ladder ~ log_gdp_per_capita + healthy_life_expectancy_at_birth, data = train_data)

#Summary of the model
mosaic::msummary(model2)

#RMSE on the training set
rmse_train <- train_data %>% 
  mutate(predictions = predict(model2, .)) %>% 
  summarise(
    sqrt(sum(predictions - life_ladder)**2 / n())
  ) %>% 
  pull()

cat(paste0("\nRMSE on training set is: ", round(rmse_train, 4)))

#RMSE on the testing set
rmse_test <- test_data %>% 
  mutate(predictions = predict(model2, .)) %>% 
  summarise(
    sqrt(sum(predictions - life_ladder)**2 / n())
  ) %>% 
  pull()

cat(paste0("\nRMSE on testing set is: ", round(rmse_test, 4)))
```
On adding life expectancy, we see that both predictors are significant. The explainability by the model increases to 66% and the test RMSE is 0.46.

Now, we add social_support to the model.


```{r, model3}
#Linear regression model
model3 <- lm(life_ladder ~ log_gdp_per_capita + healthy_life_expectancy_at_birth + social_support, data = train_data)

#Summary of the model
mosaic::msummary(model3)

#RMSE on the training set
rmse_train <- train_data %>% 
  mutate(predictions = predict(model3, .)) %>% 
  summarise(
    sqrt(sum(predictions - life_ladder)**2 / n())
  ) %>% 
  pull()

cat(paste0("\nRMSE on training set is: ", round(rmse_train, 4)))

#RMSE on the testing set
rmse_test <- test_data %>% 
  mutate(predictions = predict(model3, .)) %>% 
  summarise(
    sqrt(sum(predictions - life_ladder)**2 / n())
  ) %>% 
  pull()

cat(paste0("\nRMSE on testing set is: ", round(rmse_test, 4)))
```
Adding social support renders log GDP per capita insignificant as 95% confidence level, as the p-value for log GDP is greater than 0.05. However, the Adjusted R squared increases to 71% and the test RMSE decreases to 0.35.

In the next iteration, we add freedom_to_make_life_choices to our model.


```{r, model4}
#Linear regression model
model4 <- lm(life_ladder ~ log_gdp_per_capita + healthy_life_expectancy_at_birth + social_support + freedom_to_make_life_choices, data = train_data)

#Summary of the model
mosaic::msummary(model4)

#RMSE on the training set
rmse_train <- train_data %>% 
  mutate(predictions = predict(model4, .)) %>% 
  summarise(
    sqrt(sum(predictions - life_ladder)**2 / n())
  ) %>% 
  pull()

cat(paste0("\nRMSE on training set is: ", round(rmse_train, 4)))

#RMSE on the testing set
rmse_test <- test_data %>% 
  mutate(predictions = predict(model4, .)) %>% 
  summarise(
    sqrt(sum(predictions - life_ladder)**2 / n())
  ) %>% 
  pull()

cat(paste0("\nRMSE on testing set is: ", round(rmse_test, 4)))
```

We observe that all the variables are significant at 5% confidence level. The R squared increase to 74% and the RMSE on the test set is 0.34.

Now, we try adding positive_affect to our model and analyze the performance.

```{r, model5}
#Linear regression model 
model5 <- lm(life_ladder ~ log_gdp_per_capita + healthy_life_expectancy_at_birth + social_support + freedom_to_make_life_choices + positive_affect, data = train_data)

#Summary of the model
mosaic::msummary(model5)

#RMSE on the training set
rmse_train <- train_data %>% 
  mutate(predictions = predict(model5, .)) %>% 
  summarise(
    sqrt(sum(predictions - life_ladder)**2 / n())
  ) %>% 
  pull()

cat(paste0("\nRMSE on training set is: ", round(rmse_train, 4)))

#RMSE on the testing set
rmse_test <- test_data %>% 
  mutate(predictions = predict(model5, .)) %>% 
  summarise(
    sqrt(sum(predictions - life_ladder)**2 / n())
  ) %>% 
  pull()

cat(paste0("\nRMSE on testing set is: ", round(rmse_test, 4)))
```
We observe that positive_affect is not significant at 95% confidence level as its p-value is greater than 0.05. There is no signficant change in Adjusted R squared from our previous model but the test RMSE increases to 0.38. Hence, we drop the positive_affect variable.


Instead of positive_affect, we try the negative_affect variable in our model now.

```{r, model6}
#Linear regression model
model6 <- lm(life_ladder ~ log_gdp_per_capita + healthy_life_expectancy_at_birth + social_support + freedom_to_make_life_choices + negative_affect, data = train_data)

#Summary of the model
mosaic::msummary(model6)

#RMSE on the training set
rmse_train <- train_data %>% 
  mutate(predictions = predict(model6, .)) %>% 
  summarise(
    sqrt(sum(predictions - life_ladder)**2 / n())
  ) %>% 
  pull()

cat(paste0("\nRMSE on training set is: ", round(rmse_train, 4)))

#RMSE on the testing set
rmse_test <- test_data %>% 
  mutate(predictions = predict(model6, .)) %>% 
  summarise(
    sqrt(sum(predictions - life_ladder)**2 / n())
  ) %>% 
  pull()

cat(paste0("\nRMSE on testing set is: ", round(rmse_test, 4)))
```

We observe that adding negative_affect variable renders life expectancy as insignificant as its p-value is greater than 0.05. The test RMSE increases to 0.64 and hence, we drop the negative_affect variable.

In our last iteration, we add continent variable to our model.

```{r, model7}

#Linear regression model 
model7 <- lm(life_ladder ~ log_gdp_per_capita + healthy_life_expectancy_at_birth + social_support + freedom_to_make_life_choices + continent, data = train_data)

#Summary of the model
mosaic::msummary(model7)

#RMSE on the training set
rmse_train <- train_data %>% 
  mutate(predictions = predict(model7, .)) %>% 
  summarise(
    sqrt(sum(predictions - life_ladder)**2 / n())
  ) %>% 
  pull()

cat(paste0("\nRMSE on training set is: ", round(rmse_train, 4)))

#RMSE on the testing set
rmse_test <- test_data %>% 
  mutate(predictions = predict(model7, .)) %>% 
  summarise(
    sqrt(sum(predictions - life_ladder)**2 / n())
  ) %>% 
  pull()

cat(paste0("\nRMSE on testing set is: ", round(rmse_test, 4)))
```
We observe that this model has the highest Adjusted R squared (77.6%) out of all the models and the least RMSE on test data (0.30). We see that out of the dummy variables for continents, only continentAsia is statistically significant. 

```{r, comparison_of_models}
#Produce summary table comparing models using huxtable::huxreg()
huxreg(model1, model2, model3, model4, model7,
       statistics = c('#observations' = 'nobs',
                      'Adj. R Squared' = 'adj.r.squared', 
                      'Residual SE' = 'sigma'), 
       bold_signif = 0.05, 
       stars = NULL
) %>% 
  set_caption('Comparison of models')

```

Again, we observe that model7 outperforms the other models. We check for collinearity between the variables using Variance Inflation Factor.

```{r, vif}
#Calculating the variance inflation factor to check collinearity
car::vif(model7)
```
Looking at the VIF, we conclude that there is no significant collinearity between the variables. We now do residual analysis.


```{r, residual_analysis}
library(ggfortify) #Package for residual analysis

#Plotting graphs for residual analysis
autoplot(model7) +
  #Using black and white theme
  theme_bw()
```

We observe in the Residuals vs Fitted graph that there is no signifcant pattern observed. From the Normal Q-Q plot, we see that most of the observations within -2 and +2 lie on the normal line.


Hence, we conclude that to predict life_ladder score, a linear regression model with log_gdp_per_capita, healthy_life_expectancy_at_birth, social_support, freedom_to_make_life_choices and continent as explanatory variables performs the best. This model is able to explain 78% of the variability and the RMSE on testing data is 0.3.

